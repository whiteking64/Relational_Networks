{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort-of-CELVR\n",
    "忠実に再現したモデル(論文p.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの訓練とテスト，重みの保存を定義\n",
    "main.pyで主に使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, args, name):\n",
    "        super(BasicModel, self).__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def train_(self, input_img, input_qst, label):\n",
    "        self.optimizer.zero_grad() #全ての変数の勾配を初期化\n",
    "        output = self(input_img, input_qst)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct = pred.eq(label.data).cpu().sum()\n",
    "        return correct/len(label)*100.\n",
    "\n",
    "    def test_(self, input_img, input_qst, label):\n",
    "        output = self(input_img, input_qst)\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct = pred.eq(label.data).cpu().sum()\n",
    "        return correct/len(label)*100.\n",
    "\n",
    "    def save_model(self, epoch):\n",
    "        torch.save(self.state_dict(), 'model/epoch_{}_{:02d}.pth'.format(self.name, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下，モデルの定義\n",
    "1. ConvInputModel: 画像処理用のCNN\n",
    "2. FCOutputModel: RNのfの2層目，3層目を定義\n",
    "3. RN: CNNの最終出力の読み込みからfによるanswer_vecの出力までを実装，BasicModelを継承\n",
    "4. CNN_MLP: CNN+RNとの比較対象，BasicModelを継承"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ConvInputModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvInputModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(256)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"convolution\"\"\"\n",
    "        x = self.conv1(img)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FCOutputModel\n",
    "関数fの2-4層目を定義\n",
    "CNN+RNとその比較用のCNN+MLPで同じものを使うため，このように個別にクラスを用意している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCOutputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCOutputModel, self).__init__()\n",
    "\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        self.fc3 = nn.Linear(500, 100)\n",
    "        self.fc4 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RN\n",
    "forward関数内の流れ\n",
    "1. 画像をCNNに通す\n",
    "2. 256の5x5特徴マップに一座標を表すマップを2枚追加\n",
    "3. 質問ベクトルを追加するためにテンソルを作る\n",
    "4. それらを結合し，RNに入力するためのオブジェクトを作成する\n",
    "5. gに通し，結果を要素ごとに足し算する\n",
    "6. fに通す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN(BasicModel):\n",
    "    def __init__(self, args):\n",
    "        super(RN, self).__init__(args, 'RN')\n",
    "        #まずCNNに通す\n",
    "        self.conv = ConvInputModel()\n",
    "        \n",
    "        #gの定義\n",
    "        #(number of filters per object+coordinate of object)*2+question vector\n",
    "        self.g_fc1 = nn.Linear((256+2)*2+11, 2000) #左の数字はforwardのx_fullと対応\n",
    "        self.g_fc2 = nn.Linear(2000, 2000)\n",
    "        self.g_fc3 = nn.Linear(2000, 2000)\n",
    "        self.g_fc4 = nn.Linear(2000, 2000)\n",
    "\n",
    "        #fの定義(1層目)\n",
    "        self.f_fc1 = nn.Linear(2000, 1000)\n",
    "\n",
    "        # 座標テンソルの用意\n",
    "        def cvt_coord(i):\n",
    "            return [(i/5-2)/2., (i%5-2)/2.]\n",
    "\n",
    "        self.coord_tensor = torch.FloatTensor(args.batch_size, 25, 2)\n",
    "        if args.cuda:\n",
    "            self.coord_tensor = self.coord_tensor.cuda()\n",
    "        self.coord_tensor = Variable(self.coord_tensor)\n",
    "        np_coord_tensor = np.zeros((args.batch_size, 25, 2))\n",
    "        for i in range(25):\n",
    "            np_coord_tensor[:, i, :] = np.array(cvt_coord(i))\n",
    "        self.coord_tensor.data.copy_(torch.from_numpy(np_coord_tensor))\n",
    "\n",
    "        # fの2-4層目\n",
    "        self.fcout = FCOutputModel()\n",
    "\n",
    "        #optimizerの定義\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
    "\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        #画像をCNNに通したあとの出力, (64 x 256 x 5 x 5)\n",
    "        x = self.conv(img)\n",
    "\n",
    "        #gに通す用意\n",
    "        mb = x.size()[0] #64\n",
    "        n_channels = x.size()[1] #256\n",
    "        d = x.size()[2] #5\n",
    "        x_flat = x.view(mb, n_channels, d*d).permute(0, 2, 1) #(64 x 256 x 5 x 5) -> (64 x 25 x 256)\n",
    "        # add coordinates\n",
    "        x_flat = torch.cat([x_flat, self.coord_tensor], 2) #(64x25x(256 + 2)), 座標テンソルを列方向に結合している\n",
    "\n",
    "        # 質問テンソルの用意\n",
    "        qst = torch.unsqueeze(qst, 1) #(64x1x11) tensor 次元が1の方向に1上がったテンソルができる\n",
    "        qst = qst.repeat(1, 25, 1) #同じ(64x25x11)tensorが下に25個できる np.tileみたいな感じ\n",
    "        qst = torch.unsqueeze(qst, 2) #(64x25x1x11)\n",
    "\n",
    "        # cast all pairs against each other\n",
    "        \"\"\"\n",
    "        一つのオブジェクト = (o_i, o_j, qst)\n",
    "        x_i <- o_i (論文中)\n",
    "        x_j <- o_j + qst\n",
    "        \"\"\"\n",
    "        x_i = torch.unsqueeze(x_flat, 1) # (64x1x25x258)\n",
    "        x_i = x_i.repeat(1, 25, 1, 1) # (64x25x25x258)\n",
    "\n",
    "        x_j = torch.unsqueeze(x_flat, 2) # (64x25x1x258)\n",
    "        x_j = torch.cat([x_j, qst], 3) # (64x25x1x(258+11))\n",
    "        x_j = x_j.repeat(1, 1, 25, 1) # (64x25x25x(258+11))\n",
    "\n",
    "        # concatenate all together\n",
    "        #オブジェクトをd*dの数だけ作ったので，RNの入力用にすべて結合\n",
    "        x_full = torch.cat([x_i, x_j], 3) # (64x25x25x(258+258+11))\n",
    "\n",
    "        # gに通す\n",
    "        x_ = x_full.view(mb*d*d*d*d, 527) #RNの最初のNNに入力するため形を変形\n",
    "        x_ = self.g_fc1(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc2(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc3(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc4(x_)\n",
    "        x_ = F.relu(x_) #この段階での形は，(mb(=64) x d x d x d x d x 2000)\n",
    "\n",
    "        #gの出力をsummation\n",
    "        x_g = x_.view(mb, d*d*d*d, 2000)\n",
    "        x_g = x_g.sum(1).squeeze() #element-wise sum\n",
    "\n",
    "        #fに通す\n",
    "        x_f = self.f_fc1(x_g) #fの1層目\n",
    "        x_f = F.relu(x_f)\n",
    "\n",
    "        return self.fcout(x_f) #fの2層目以降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_MLP\n",
    "RNとの比較用のアーキテクチャ．\n",
    "本来はパラメータ数がもっと多いが，ここではMLPにRNで用いたfのMLPの2-4層を再利用している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_MLP(BasicModel):\n",
    "    def __init__(self, args):\n",
    "        super(CNN_MLP, self).__init__(args, 'CNNMLP')\n",
    "\n",
    "        self.conv = ConvInputModel()\n",
    "        self.fc1 = nn.Linear(5*5*256 + 11, 1000)  # question concatenated to all\n",
    "        self.fcout = FCOutputModel()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=args.lr)\n",
    "\n",
    "    def forward(self, img, qst):\n",
    "        x = self.conv(img) # x = (64 x 256 x 5 x 5)\n",
    "        #fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_ = torch.cat((x, qst), 1)  # Concat question\n",
    "        x_ = self.fc1(x_)\n",
    "        x_ = F.relu(x_)\n",
    "\n",
    "        return self.fcout(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
